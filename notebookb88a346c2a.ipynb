{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\nsentence = [\"What\", \"are\", \"the\", \"symptoms\", \"of\", \"diabetes\", \"?\"]\nd_model = 8  \nseq_len = len(sentence)\n\ntorch.manual_seed(42)\nembeddings = torch.rand((seq_len, d_model))  \n\ndef self_attention(Q, K, V):\n    d_k = Q.shape[-1]  \n    attention_scores = torch.matmul(Q, K.T) / (d_k ** 0.5)  \n    attention_weights = F.softmax(attention_scores, dim=-1)  \n    output = torch.matmul(attention_weights, V)  \n    return output, attention_weights\n\nQ, K, V = embeddings, embeddings, embeddings\noutput, attention_weights = self_attention(Q, K, V)\n\nprint(\"Self-Attention Scores:\")\nprint(attention_weights)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T23:37:01.341412Z","iopub.execute_input":"2025-03-29T23:37:01.341769Z","iopub.status.idle":"2025-03-29T23:37:06.010100Z","shell.execute_reply.started":"2025-03-29T23:37:01.341730Z","shell.execute_reply":"2025-03-29T23:37:06.008754Z"}},"outputs":[{"name":"stdout","text":"Self-Attention Scores:\ntensor([[0.1949, 0.1468, 0.1482, 0.0750, 0.1640, 0.1537, 0.1174],\n        [0.1463, 0.1976, 0.1227, 0.0929, 0.1565, 0.1724, 0.1115],\n        [0.1797, 0.1493, 0.1510, 0.0839, 0.1562, 0.1505, 0.1293],\n        [0.1330, 0.1654, 0.1228, 0.1465, 0.1437, 0.1672, 0.1214],\n        [0.1695, 0.1623, 0.1331, 0.0837, 0.1735, 0.1538, 0.1241],\n        [0.1578, 0.1776, 0.1274, 0.0968, 0.1528, 0.1809, 0.1068],\n        [0.1575, 0.1500, 0.1430, 0.0917, 0.1610, 0.1395, 0.1574]])\n","output_type":"stream"}],"execution_count":1}]}